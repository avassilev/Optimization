% !TeX spellcheck = en_GB
\documentclass[10pt]{beamer}
\usetheme{CambridgeUS}
%\usetheme{Boadilla}
\definecolor{myred}{RGB}{163,0,0}
%\usecolortheme[named=blue]{structure}
\usecolortheme{dove}
\usefonttheme[]{professionalfonts}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{gensymb}
\usepackage{verbatim} 
\usepackage{paratype}
\usepackage{mathpazo}
\usepackage{listings}
\lstset{language=Python}

\usepackage{tikz}
\usetikzlibrary{matrix}

\DeclareMathOperator*{\interior}{int}

% Number theorem environments
\setbeamertemplate{theorem}[ams style]
\setbeamertemplate{theorems}[numbered]

% Reset theorem-like environments so that each is numbered separately
\usepackage{etoolbox}
\undef{\definition}
\theoremstyle{definition}
\newtheorem{definition}{\translate{Definition}}
\newtheorem{Fact}{\translate{Fact}}

% Change colours for theorem-like environments
\definecolor{mygreen1}{RGB}{0,96,0}
\definecolor{mygreen2}{RGB}{229,239,229}
\setbeamercolor{block title}{fg=white,bg=mygreen1}
\setbeamercolor{block body}{fg=black,bg=mygreen2}



\alt<presentation>
{\lstset{%
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\slshape\color{green!50!black},
  frame = single,  
  keywordstyle=\bfseries\color{blue!50!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  %escapechar=\#,
  showstringspaces = false,
  showtabs = false,
  tabsize = 2,
  emphstyle=\color{red}}
}
{
  \lstset{%
    basicstyle=\ttfamily,
    keywordstyle=\bfseries,
    commentstyle=\itshape,
    escapechar=\#,
    showtabs = false,
	tabsize = 2,
    emphstyle=\bfseries\color{red}
  }
} 

\title{R401: Statistical and Mathematical Foundations}
\subtitle{Lecture 19: Infinite-Horizon Deterministic Optimal Control in Discrete Time}
\author{Andrey Vassilev}

\date{2016/2017} 
    
\AtBeginSection{\frame{\usebeamerfont{section title}\centering\insertsection}}

\begin{document}
\maketitle



\begin{frame}[fragile]
\frametitle{Lecture Contents}
\tableofcontents
\end{frame}

\begin{section}{Introduction}\label{sec:intro}

\begin{frame}[fragile]
\frametitle{Switching to discrete time}
\begin{itemize}\itemsep1em
\item You are already familiar with a number of dynamic optimization problems in which time is continuous.
\item In many applications, however, it is natural to work in discrete time.
\item This provides a bridge to validating/calibrating models with data or estimating them, among others.
\item We therefore need to develop the counterpart of the continuous-time optimal control framework for the case of discrete time.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Switching to discrete time}
\begin{itemize}\itemsep1em
\item Some of the details of such a transition are easily predictable:
	\begin{itemize}
	\item Differential equations will be replaced by difference equations.
	\item The objective functional will involve a series instead of an integral.
	\end{itemize}
\item Other details need to be specified further. In particular, there exist two broad classes of dynamic optimization problems in discrete time:
	\begin{itemize}
	\item Problems in variational form
	\item Problems with explicit controls
	\end{itemize}
\item Both classes can be used to address a wide variety of problems. 
\item However, problems with explicit controls are a bit more transparent in terms of their structure.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Specific assumptions}
\begin{itemize}\itemsep1em
\item We will work in an infinite-horizon setup. Finite-horizon formulations for discrete-time problems exist but are less common in economic applications.
\item We sacrifice some generality from the outset by assuming a specific structure of the problems:
	\begin{itemize}
	\item Special (time) discounting in the objective functionals.
	\item Autonomous difference equations describing the evolution of the system that is being modelled.
	\end{itemize}
\end{itemize}
\end{frame}


\end{section}

\begin{section}{Problems in variational form}\label{sec:varprob}

\begin{frame}[fragile]
\frametitle{Formulation}
\framesubtitle{}

\end{frame}

\begin{frame}[fragile]
\frametitle{The Euler equation}
\framesubtitle{}

\end{frame}

\begin{frame}[fragile]
\frametitle{Sufficiency}
\framesubtitle{}

\end{frame}



\begin{frame}[fragile]
\frametitle{Example: NCs for a problem in variational form}
Compute the NCs for the problem:
\[ \max_{\{c_t\}} \sum_{t=0}^{\infty}\beta^t \ln c_t  \]
\[ k_{t+1} = (1-\delta)k_t + y_t - c_t,\quad k_0>0 \text{ -- given} \] \[ y_t=A k_t^\alpha , \quad 0<\underline{\epsilon} \leq c_t \leq \overline{\epsilon}, \quad \alpha,\beta \in (0,1), \quad A>0\]

NCs: 
\[ \sum_{t=0}^{\infty}\beta^t \underbrace{\ln ((1-\delta)k_t + A k_t^\alpha - k_{t+1})}_{= F(x_t,x_{t+1})}. \]

{\color{red}Euler equation: \[ F_y(x_t,x_{t+1})+\beta F_x(x_{t+1},x_{t+2})=0 \]}
\end{frame}



\begin{frame}[fragile]
\frametitle{Example: NCs for a problem in variational form}
Applying the Euler equation, we get:
\[ \frac{-1}{(1-\delta)k_t +A k_t^\alpha -k_{t+1}} + \beta \frac{(1-\delta) + \alpha A k_{t+1}^{\alpha -1}}{(1-\delta)k_{t+1} +A k_{t+1}^\alpha -k_{t+2}} = 0 \]

The last result has the form 
\[ G(x_{t+2},x_{t+1},x_t)=0, \]
i.e. a nonlinear second-order difference equation.

\pause \bigskip
\textbf{Question:} Can you compute the solution of the equation for $ \beta = 0.95,~A=1,~ \delta = 0.05,~ \alpha = 0.5 $ and $ k_0 = 2 $? \pause If not, what else do you need?
\end{frame}

\end{section}

\begin{section}{Problems with explicit controls}\label{sec:explcontr}

\begin{frame}[fragile]
\frametitle{Formulation of the problem with explicit controls}
$X \subset \mathbb{R}^n$  -- \emph{state space}, the state variables are $x=(x^1,\ldots,x^n)$. \bigskip

We assume that $\forall x \in X,~ \exists U(x) \subset \mathbb{R}^m, U(x)\neq \emptyset$. The elements of $u=(u^1,\ldots,u^m)$ are our \emph{controls}. \bigskip

Objective function (instantaneous): $f^0(x,u)$ лил░ $x \in X$, $u \in U(x)$ \bigskip

State equations:
\begin{equation}
x_{t+1}=f(x_t,u_t),\quad x_0 \text{ -- given} \label{eq:state}
\end{equation}
where $f(x,u)$ is a vector function taking values in $X$, for $x \in X$, $u \in U(x)$.\bigskip
\end{frame}



\begin{frame}[fragile]
\frametitle{Formulation of the problem with explicit controls}

We need to find a sequence of admissible controls $\mathbf{u} = \{u_t\}$, $t=0,1,2,\ldots,$
which determine a sequence of state variables $\{ x_{t+1} \}$, $t=0,1,2,\ldots$ via \eqref{eq:state} for which \begin{equation}
j(x_0,\mathbf{u})=\sum_{t=0}^\infty
\beta^t f^0(x_t,u_t)\label{eq:obj}
\end{equation} attains a maximum \begin{equation}
v(x_0)=\sup_{\mathbf{u}} j(x_0,\mathbf{u}).  
\label{eq:objMax}
\end{equation} 

We refer to the above problem as \emph{Problem A}.
\end{frame}



\begin{frame}[fragile]
\frametitle{Formulation of the problem with explicit controls}
The number $\beta \in (0,1)$ is the \emph{discount factor} in the model. \bigskip

Denote by $\textrm{FC}(x_0)$ the set of all feasible control sequences $\{ u_t\}_{t=0}^\infty$ for initial $x_0 \in X$, i.e. $ x_{t+1} $
satisfies \eqref{eq:state} for $ u_t \in U(x_t)$, $t=0,1,2,\ldots,$
and a given $x_0$. \bigskip

We denote the optimal sequence of pairs of state variables and controls for Problem A by $\{x^*_{t+1},u^*_t\},~t=0,1,2,\ldots$, i.e. $\{u^*_t\} \in \textrm{FC}(x_0)$, and $$v(x_0)=j(x_0,\mathbf{u^*}),~\mathbf{u^*}=\{u^*_t\}.$$
\end{frame}



\begin{frame}[fragile]
\frametitle{Necessary conditions for optimality}
A variation of the familiar approach for problems of this type is the following:\begin{enumerate}
\item Construct the Lagrangian
\[\mathcal{L}(x_1,x_2,\ldots,u_0,u_1,\ldots) =
 \sum_{t=0}^{\infty}\beta^t \left[ f^0(x_t,u_t) + \lambda'_t \cdot
[f(x_t,u_t)-x_{t+1}] \right],  \] where $\lambda_t=(\lambda^1_t,\ldots,\lambda^n_t)$, $t=0,1,2,\ldots,$ are the Lagrange multipliers and the dot ($ \cdot $) denotes a matrix product or a scalar product: $$\lambda'_t \cdot [f(x_t,u_t)-x_{t+1}]
= \sum_{i=1}^n \lambda_t^i [f^i(x_t,u_t)-x^i_{t+1}].$$
\end{enumerate}
\end{frame}



\begin{frame}[fragile]
\frametitle{Necessary conditions for optimality}
\begin{enumerate} \setcounter{enumi}{1}
\item Differentiate $\mathcal{L}$ w.r.t. $x_t$ and $u_t$, set the resulting expressions equal to zero and obtain first-order necessary conditions for optimality: \begin{equation}\begin{split}
& \beta \left[ f^0_{x_t^k}(x_t,u_t)+\sum_{i=1}^n \lambda_t^i f^i_{x_t^k}(x_t,u_t)\right] = \lambda^k_{t-1},~ k=1,\ldots,n, \\
& f^0_{u_t^j}(x_t,u_t)+ \sum_{i=1}^n \lambda_t^i f^i_{u_t^j}(x_t,u_t) = 0,~ j=1,\ldots,m.
\end{split}
\label{eq:FOCs}
\end{equation}
\item Equations \eqref{eq:state} and \eqref{eq:FOCs} are solved as a system and we obtain a candidate solution $\{u_t\}_{t=0}^\infty$ or, more precisely, a sequence
$\{x_{t+1},u_t\}_{t=0}^\infty$. (It is common to find a stationary point of the system \eqref{eq:state} and \eqref{eq:FOCs}, and work with a linearised version of the system around that point.)
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Necessary conditions for optimality}
In matrix notation the above takes that form:
\[ \mathcal{L}_x = \beta^t f^0_x(x_t,u_t)+\beta^t f'_x(x_t,u_t)\cdot \lambda_t - \beta^{t-1}\lambda_{t-1} = 0 \Rightarrow \]
\begin{equation}
\beta ( f^0_x(x_t,u_t)+ f'_x(x_t,u_t)\cdot \lambda_t ) = \lambda_{t-1}.
\label{eq:LagrX}
\end{equation}  \bigskip
\[ \mathcal{L}_u = \beta^t f^0_u (x_t,u_t)+ \beta^t f'_u(x_t,u_t)\cdot \lambda_t = 0 \Rightarrow \]
\begin{equation}
f^0_u (x_t,u_t)+ f'_u(x_t,u_t)\cdot \lambda_t = 0
\label{eq:LagrU}
\end{equation}
\end{frame}



\begin{frame}[fragile]
\frametitle{Necessary conditions for optimality}
\textbf{Why is this algorithm valid?}\bigskip \bigskip

The value function for Problem A satisfies a version of the Bellman equation:
\begin{equation}
v(x) = \sup_{u \in U(x)}  \left[
f^0(x,u)+\beta v(f(x,u)) \right]. \label{eq:Bellman}
\end{equation}


Let the supremum in \eqref{eq:Bellman} be attained on the interior of the set $ U(x) $. Denote this point by $ u = \nu(x) $ and assume that all objects used below are differentiable.
\end{frame}



\begin{frame}[fragile]
\frametitle{Necessary conditions for optimality}
We have \begin{equation}
 v(x) = f^0(x,\nu(x)) + \beta v(f(x,\nu(x))) .
\label{eq:Bellman1}
\end{equation}

Also the extremum condition is \begin{equation}
f^0_u(x,\nu(x)) + \beta f'_u(x,\nu(x)) \cdot v_x(f(x,\nu(x))) = 0.
\label{eq:diffCntrl}
\end{equation} 

Differentiating \eqref{eq:Bellman1} w.r.t. $ x $, we obtain
\begin{equation*}
\begin{split}
 v_x(x) = & f^0_x(x,\nu(x)) +  \nu'_x(x) \cdot f^0_u(x,\nu(x)) + \\
 & \beta \left[ f'_x(x,\nu(x)) + \nu'_x(x) \cdot f'_u(x,\nu(x))  \right]\cdot v_x(f(x,\nu(x)))\\
 = & f^0_x(x,\nu(x)) + \beta  f'_x(x,\nu(x))\cdot v_x(f(x,\nu(x))) + \\
 &  \underbrace{\nu'_x(x) \cdot f^0_u(x,\nu(x)) + \beta \nu'_x(x) \cdot f'_u(x,\nu(x)) \cdot v_x(f(x,\nu(x)))}_{=0 \text{ in view of \eqref{eq:diffCntrl}}}.
\end{split}
\end{equation*}
\end{frame}


\begin{frame}[fragile]
\frametitle{Necessary conditions for optimality}
We thus get \begin{equation}
v_x(x) = f^0_x(x,\nu(x)) + \beta  f'_x(x,\nu(x))\cdot v_x(f(x,\nu(x))).
\label{eq:Bellman1dx1}
\end{equation}
For $ x=x^*_t $ and $ u^*_t = \nu(x^*_t) $, equations \eqref{eq:diffCntrl} and \eqref{eq:Bellman1dx1} take the form
\begin{equation}
f^0_u(x^*_t,u^*_t) + \beta f'_u(x^*_t,u^*_t) \cdot v_x(x^*_{t+1}) = 0,
\label{eq:diffCntrlA} 
\end{equation}
\begin{equation}
v_x(x^*_t) = f^0_x(x^*_t,u^*_t) + \beta  f'_x(x^*_t,u^*_t)\cdot v_x(x^*_{t+1}).
\label{eq:Bellman1dx1A}
\end{equation}

Set $ \lambda_t := \beta v_x(x^*_{t+1}) $ in \eqref{eq:diffCntrlA} and \eqref{eq:Bellman1dx1A}, to obtain precisely \eqref{eq:LagrX} and \eqref{eq:LagrU}.
\end{frame}


\begin{frame}[fragile]
\frametitle{Sufficient conditions}
\begin{Fact} Let $\{\lambda_t\}$ and $\{x^*_{t+1},u^*_t\}$,
$t=0,1,2,\ldots,$ be obtained by using \eqref{eq:state}
and \eqref{eq:FOCs}. If 
	\begin{enumerate}
	\item The functions $f^0(x,u)$ and $f(x,u)$
	are concave in $(x,u)$,
	\item The Lagrange multipliers $\lambda_t^1,\ldots,\lambda_t^n,
	~t=0,1,2,\ldots$ are nonnegative,
	\item The state space $X$ is a subset of $\mathbb{R}^n_+$ and the following transversality condition is valid $$\lim_{T\rightarrow \infty} \beta^T \lambda'_T \cdot
	x^*_{T+1}=0,$$
	\end{enumerate} 
then the sequence $\{x^*_{t+1},u^*_t\}$ (for a given $x_0$) is optimal for Problem A.
\label{fc:sufficiency}\end{Fact}
\end{frame}


\begin{frame}[fragile]
\frametitle{Sufficient conditions}
We shall verify the validity of Fact \ref{fc:sufficiency}.\bigskip

Recall that \eqref{eq:FOCs} in matrix terms is given by \eqref{eq:LagrX} and \eqref{eq:LagrU}.

Consider \[ \mathcal{L}_T(x_t,u_t) = \sum_{t=0}^{T}\beta^t \left\{ f^0(x_t,u_t) + \lambda'_t \cdot [f(x_t,u_t)-x_{t+1}] \right\} .\]

We have \begin{equation}
\begin{split}
& D := \mathcal{L}_T (x_t,u_t) - \mathcal{L}_T(x^*_t,u^*_t) = \sum_{t=0}^{T}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1} )+  \\
& \sum_{t=0}^{T}\beta^t [ f^0(x_t,u_t) + \lambda'_t \cdot f(x_t,u_t) 
- f^0(x^*_t,u^*_t) - \lambda'_t \cdot f(x^*_t,u^*_t) ].
\end{split}
\label{eq:LagrDiff}
\end{equation}
\end{frame}


\begin{frame}[fragile]
\frametitle{Sufficient conditions}
Then, in view of concavity, we get \begin{equation*}
\begin{split}
&\mathcal{L}_T (x_t,u_t) - \mathcal{L}_T(x^*_t,u^*_t) ~~(= D)~~ \leq \sum_{t=0}^{T}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1} )+  \\
&\sum_{t=0}^{T}\beta^t [ f^{0'}_x(x^*_t,u^*_t)\cdot(x_t-x_t^*) +f^{0'}_u(x^*_t,u^*_t)\cdot(u_t-u_t^*) +\\
& \lambda'_t \cdot [f_x(x^*_t,u^*_t)\cdot(x_t-x_t^*) + f_u(x^*_t,u^*_t)\cdot (u_t-u_t^*) ]] = \\
& \sum_{t=0}^{T}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1} )+  \sum_{t=0}^{T}\beta^t \underbrace{[f^{0'}_x(x^*_t,u^*_t) + \lambda'_t \cdot f_x(x^*_t,u^*_t)]}_{=\frac{\lambda'_{t-1}}{\beta} \text{ in view of }  \eqref{eq:LagrX} }\cdot(x_t-x_t^*)\\
& + \sum_{t=0}^{T}\beta^t \underbrace{[f^{0'}_u(x^*_t,u^*_t) + \lambda'_t \cdot f_u(x^*_t,u^*_t)]}_{=\mathbf{0}' \text{ in view of } \eqref{eq:LagrU}}\cdot(u_t-u_t^*).
\end{split}
\end{equation*}
\end{frame}



\begin{frame}[fragile]
\frametitle{Sufficient conditions}
I.e. \begin{equation*}
\begin{split}
 D \leq &   \sum_{t=0}^{T}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1} ) +  \sum_{t=0}^{T}\beta^t \frac{\lambda'_{t-1}}{\beta}\cdot\underbrace{(x_t-x_t^*)}_{\text{N.B.: } x_0 = x^*_0} = \\
&  \sum_{t=0}^{T}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1} ) +  \sum_{t = {\color{red}1}}^{T}\beta^{t-1} \frac{\lambda'_{t-1}}{\beta}\cdot(x_t-x_t^*)= \\
&  \sum_{t=0}^{T}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1} ) +  \sum_{t = {\color{red} 0}}^{{\color{red} T-1}}\beta^{{\color{red} t}} \frac{\lambda'_{{\color{red} t}}}{\beta}\cdot(x_{{\color{red} t+1}}-x_{{\color{red} t+1}}^*)= \\
& \underbrace{\sum_{t=0}^{{\color{red} T-1}}\beta^t \lambda'_t \cdot (x^*_{t+1} - x_{t+1}) +  \sum_{t = 0}^{T-1}\beta^{t} \frac{\lambda'_{t}}{\beta}\cdot(x_{ t+1}-x_{t+1}^*)}_{=0} + \\
& \beta^T \lambda'_T \cdot (x^*_{T+1} - x_{T+1})  \underbrace{\leq}_{\text{since }\lambda_t,x_t \geq \mathbf{0}} \beta^T \lambda'_T \cdot x^*_{T+1}.
\end{split}
\end{equation*}
\end{frame}


\begin{frame}[fragile]
\frametitle{Sufficient conditions}
In view of the transversality condition, we have: \begin{equation*}
 D \leq  \beta^T \lambda'_T \cdot x^*_{T+1} ~ \mathop{\longrightarrow}_{T \rightarrow \infty} ~0 ,
\end{equation*}
i.e. 
\[  \mathcal{L}_T(x^*_t,u^*_t) - \mathcal{L}_T (x_t,u_t) \geq 0, \] which proves the optimality of the sequence
$\{x^*_{t+1},u^*_t\}$.
\end{frame}
\end{section}

\begin{frame}[fragile]
\frametitle{Homework}
\framesubtitle{}

\end{frame}


\begin{frame}[fragile]
\frametitle{Readings}
Main references:\bigskip
Additional readings:\bigskip

Syds\ae{}ter et al. [SHSS] \emph{Further mathematics for economic analysis}. Chapter 12.\bigskip
\end{frame}

\end{document}

